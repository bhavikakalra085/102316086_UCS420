{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhavikakalra085/102316086_UCS420/blob/main/assignmnet9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17c2ede3-313f-4488-8065-553006a1efa6",
      "metadata": {
        "id": "17c2ede3-313f-4488-8065-553006a1efa6"
      },
      "source": [
        "# Question 1\n",
        "### Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports, technology, food, books, etc.).\n",
        "1. Convert text to lowercase and remove punctuaƟon.\n",
        "2. Tokenize the text into words and sentences.\n",
        "3. Remove stopwords (using NLTK's stopwords list).\n",
        "4. Display word frequency distribuƟon (excluding stopwords)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fe0056b-d290-4f14-bb39-9684e86fdda3",
      "metadata": {
        "id": "6fe0056b-d290-4f14-bb39-9684e86fdda3",
        "outputId": "3bd9ef82-8f57-4c7d-f69a-c7eae9d88402"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in d:\\anaconda3\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in d:\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in d:\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
            "Requirement already satisfied: tqdm in d:\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: colorama in d:\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ab118907-6830-4a80-b3b5-db101ec4f2c2",
      "metadata": {
        "id": "ab118907-6830-4a80-b3b5-db101ec4f2c2"
      },
      "outputs": [],
      "source": [
        "sentence = \"\"\"\n",
        "Aloo ki sabzi is a simple yet comforting dish that brings warmth to any meal.\n",
        " Made with tender potatoes cooked in a flavorful blend of spices like cumin, turmeric, and coriander, it’s both hearty and satisfying.\n",
        " The potatoes soak up all the spices, creating a deliciously aromatic dish that pairs perfectly with roti, paratha, or rice.\n",
        "  Whether it's a spicy version with green chilies or a milder one with a touch of sweetness, aloo ki sabzi is loved for its versatility and comforting taste.\n",
        " It’s a dish that feels like home in every bite!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b3cd1155-77b3-41d6-902e-b4513ec1950a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3cd1155-77b3-41d6-902e-b4513ec1950a",
        "outputId": "d3e2f6f2-68b0-4a2c-8b8a-f4b0cc62be4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "aloo ki sabzi is a simple yet comforting dish that brings warmth to any meal\n",
            " made with tender potatoes cooked in a flavorful blend of spices like cumin turmeric and coriander it’s both hearty and satisfying \n",
            " the potatoes soak up all the spices creating a deliciously aromatic dish that pairs perfectly with roti paratha or rice\n",
            "  whether its a spicy version with green chilies or a milder one with a touch of sweetness aloo ki sabzi is loved for its versatility and comforting taste\n",
            " it’s a dish that feels like home in every bite\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "text_low = sentence.lower()\n",
        "no_punc = text_low.translate(str.maketrans('', '', string.punctuation))\n",
        "print(no_punc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8e71e258-fca7-45b3-8601-f8a994fc4ec8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e71e258-fca7-45b3-8601-f8a994fc4ec8",
        "outputId": "7c602c8e-0367-4ad6-ce74-1b737f58ac61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2519cfc0-eacc-430e-bff6-12762151e505",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2519cfc0-eacc-430e-bff6-12762151e505",
        "outputId": "5459e996-d8e8-43f6-d9d6-7e7728f77ba6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized words:   ['aloo', 'ki', 'sabzi', 'is', 'a', 'simple', 'yet', 'comforting', 'dish', 'that', 'brings', 'warmth', 'to', 'any', 'meal', 'made', 'with', 'tender', 'potatoes', 'cooked', 'in', 'a', 'flavorful', 'blend', 'of', 'spices', 'like', 'cumin', 'turmeric', 'and', 'coriander', 'it', '’', 's', 'both', 'hearty', 'and', 'satisfying', 'the', 'potatoes', 'soak', 'up', 'all', 'the', 'spices', 'creating', 'a', 'deliciously', 'aromatic', 'dish', 'that', 'pairs', 'perfectly', 'with', 'roti', 'paratha', 'or', 'rice', 'whether', 'its', 'a', 'spicy', 'version', 'with', 'green', 'chilies', 'or', 'a', 'milder', 'one', 'with', 'a', 'touch', 'of', 'sweetness', 'aloo', 'ki', 'sabzi', 'is', 'loved', 'for', 'its', 'versatility', 'and', 'comforting', 'taste', 'it', '’', 's', 'a', 'dish', 'that', 'feels', 'like', 'home', 'in', 'every', 'bite'] \n",
            "\n",
            "Tokenized Sentences:   ['\\nAloo ki sabzi is a simple yet comforting dish that brings warmth to any meal.', 'Made with tender potatoes cooked in a flavorful blend of spices like cumin, turmeric, and coriander, it’s both hearty and satisfying.', 'The potatoes soak up all the spices, creating a deliciously aromatic dish that pairs perfectly with roti, paratha, or rice.', \"Whether it's a spicy version with green chilies or a milder one with a touch of sweetness, aloo ki sabzi is loved for its versatility and comforting taste.\", 'It’s a dish that feels like home in every bite!'] \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.data.path.append('/root/nltk_data')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "\n",
        "words = word_tokenize(no_punc)\n",
        "sent = sent_tokenize(sentence)\n",
        "print(\"Tokenized words:  \", words,\"\\n\")\n",
        "print(\"Tokenized Sentences:  \", sent,\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "35b03bed-d826-4e1b-8b09-6bc742c95667",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35b03bed-d826-4e1b-8b09-6bc742c95667",
        "outputId": "874e379d-44fd-4f87-99bc-d6441e6ba6bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopper = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "16f21f29-e0f5-425b-ad6b-97feb0757d0e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16f21f29-e0f5-425b-ad6b-97feb0757d0e",
        "outputId": "43931aba-1640-451b-f955-66d8372f6bdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Word Tokens:\n",
            " ['aloo', 'ki', 'sabzi', 'simple', 'yet', 'comforting', 'dish', 'brings', 'warmth', 'meal', 'made', 'tender', 'potatoes', 'cooked', 'flavorful', 'blend', 'spices', 'like', 'cumin', 'turmeric', 'coriander', '’', 'hearty', 'satisfying', 'potatoes', 'soak', 'spices', 'creating', 'deliciously', 'aromatic', 'dish', 'pairs', 'perfectly', 'roti', 'paratha', 'rice', 'whether', 'spicy', 'version', 'green', 'chilies', 'milder', 'one', 'touch', 'sweetness', 'aloo', 'ki', 'sabzi', 'loved', 'versatility', 'comforting', 'taste', '’', 'dish', 'feels', 'like', 'home', 'every', 'bite'] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "stopper_removed = [w for w in words if w not in stopper]\n",
        "print(\"Filtered Word Tokens:\\n\", stopper_removed ,'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "04b7aa6f-0141-43ef-8c21-ac3fe7a8e75f",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04b7aa6f-0141-43ef-8c21-ac3fe7a8e75f",
        "outputId": "ede3d3b8-5c4d-4348-c3a9-c5bcab3a1d9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Frequency Distribution:\n",
            "aloo: 2\n",
            "ki: 2\n",
            "sabzi: 2\n",
            "simple: 1\n",
            "yet: 1\n",
            "comforting: 2\n",
            "dish: 3\n",
            "brings: 1\n",
            "warmth: 1\n",
            "meal: 1\n",
            "made: 1\n",
            "tender: 1\n",
            "potatoes: 2\n",
            "cooked: 1\n",
            "flavorful: 1\n",
            "blend: 1\n",
            "spices: 2\n",
            "like: 2\n",
            "cumin: 1\n",
            "turmeric: 1\n",
            "coriander: 1\n",
            "’: 2\n",
            "hearty: 1\n",
            "satisfying: 1\n",
            "soak: 1\n",
            "creating: 1\n",
            "deliciously: 1\n",
            "aromatic: 1\n",
            "pairs: 1\n",
            "perfectly: 1\n",
            "roti: 1\n",
            "paratha: 1\n",
            "rice: 1\n",
            "whether: 1\n",
            "spicy: 1\n",
            "version: 1\n",
            "green: 1\n",
            "chilies: 1\n",
            "milder: 1\n",
            "one: 1\n",
            "touch: 1\n",
            "sweetness: 1\n",
            "loved: 1\n",
            "versatility: 1\n",
            "taste: 1\n",
            "feels: 1\n",
            "home: 1\n",
            "every: 1\n",
            "bite: 1\n"
          ]
        }
      ],
      "source": [
        "from nltk.probability import FreqDist\n",
        "frequency_dist = FreqDist(stopper_removed)\n",
        "print(\"\\nWord Frequency Distribution:\")\n",
        "for word, frequency in frequency_dist.items():\n",
        "    print(f\"{word}: {frequency}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "910039f1-ab9c-4a29-8c5a-8c3f1a488b4c",
      "metadata": {
        "id": "910039f1-ab9c-4a29-8c5a-8c3f1a488b4c"
      },
      "source": [
        "# Question 2\n",
        "### Stemming and Lemmatization\n",
        "1. Take the tokenized words from QuesƟon 1 (aŌer stopword removal).\n",
        "2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
        "3. Apply lemmaƟzaƟon using NLTK's WordNetLemmaƟzer.\n",
        "4. Compare and display results of both techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d73d1f7d-bb88-40a6-9eda-13bd1452b3dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d73d1f7d-bb88-40a6-9eda-13bd1452b3dc",
        "outputId": "e1732b99-aeed-4324-8e36-e46db4a18fa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4') #lematizer\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "portered = [porter.stem(w) for w in stopper_removed]\n",
        "lancastered = [lancaster.stem(w) for w in stopper_removed]\n",
        "lemmatized = [lemmatizer.lemmatize(w) for w in stopper_removed]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "5dcdf1ec-56fc-496e-8efc-491dd2089bab",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dcdf1ec-56fc-496e-8efc-491dd2089bab",
        "outputId": "d710bbbd-eb90-4487-f86c-68bd36575cda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original            Porter              Lancaster           Lemma              \n",
            "------------------------------------------------------------\n",
            "aloo                aloo                aloo                aloo               \n",
            "ki                  ki                  ki                  ki                 \n",
            "sabzi               sabzi               sabz                sabzi              \n",
            "simple              simpl               simpl               simple             \n",
            "yet                 yet                 yet                 yet                \n",
            "comforting          comfort             comfort             comforting         \n",
            "dish                dish                dish                dish               \n",
            "brings              bring               bring               brings             \n",
            "warmth              warmth              warm                warmth             \n",
            "meal                meal                meal                meal               \n",
            "made                made                mad                 made               \n",
            "tender              tender              tend                tender             \n",
            "potatoes            potato              potato              potato             \n",
            "cooked              cook                cook                cooked             \n",
            "flavorful           flavor              flav                flavorful          \n",
            "blend               blend               blend               blend              \n",
            "spices              spice               spic                spice              \n",
            "like                like                lik                 like               \n",
            "cumin               cumin               cumin               cumin              \n",
            "turmeric            turmer              turm                turmeric           \n",
            "coriander           coriand             coriand             coriander          \n",
            "’                   ’                   ’                   ’                  \n",
            "hearty              hearti              hearty              hearty             \n",
            "satisfying          satisfi             satisfy             satisfying         \n",
            "potatoes            potato              potato              potato             \n",
            "soak                soak                soak                soak               \n",
            "spices              spice               spic                spice              \n",
            "creating            creat               cre                 creating           \n",
            "deliciously         delici              delicy              deliciously        \n",
            "aromatic            aromat              arom                aromatic           \n",
            "dish                dish                dish                dish               \n",
            "pairs               pair                pair                pair               \n",
            "perfectly           perfectli           perfect             perfectly          \n",
            "roti                roti                rot                 roti               \n",
            "paratha             paratha             parath              paratha            \n",
            "rice                rice                ric                 rice               \n",
            "whether             whether             wheth               whether            \n",
            "spicy               spici               spicy               spicy              \n",
            "version             version             vert                version            \n",
            "green               green               green               green              \n",
            "chilies             chili               chi                 chilies            \n",
            "milder              milder              mild                milder             \n",
            "one                 one                 on                  one                \n",
            "touch               touch               touch               touch              \n",
            "sweetness           sweet               sweet               sweetness          \n",
            "aloo                aloo                aloo                aloo               \n",
            "ki                  ki                  ki                  ki                 \n",
            "sabzi               sabzi               sabz                sabzi              \n",
            "loved               love                lov                 loved              \n",
            "versatility         versatil            versatil            versatility        \n",
            "comforting          comfort             comfort             comforting         \n",
            "taste               tast                tast                taste              \n",
            "’                   ’                   ’                   ’                  \n",
            "dish                dish                dish                dish               \n",
            "feels               feel                feel                feel               \n",
            "like                like                lik                 like               \n",
            "home                home                hom                 home               \n",
            "every               everi               every               every              \n",
            "bite                bite                bit                 bite               \n"
          ]
        }
      ],
      "source": [
        "print(f\"{'Original':<19} {'Porter':<19} {'Lancaster':<19} {'Lemma':<19}\")\n",
        "print(\"-\" * 60)\n",
        "for o, p, l, le in zip(stopper_removed, portered, lancastered, lemmatized):\n",
        "    print(\"{:<19} {:<19} {:<19} {:<19}\".format(o, p, l, le))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49f3f33a-5e64-4d4b-8ae4-6ce4e8665250",
      "metadata": {
        "id": "49f3f33a-5e64-4d4b-8ae4-6ce4e8665250"
      },
      "source": [
        "# Question 3. Regular Expressions and Text Spliƫng\n",
        "1. Take their original text from Question 1.\n",
        "2. Use regular expressions to:\n",
        "\n",
        "    - a. Extract all words with more than 5 letters.\n",
        "\n",
        "    - b. Extract all numbers (if any exist in their text).\n",
        "\n",
        "    - c. Extract all capitalized words.\n",
        "3. Use text spliƫng techniques to:\n",
        "   \n",
        "    - a. Split the text into words containing only alphabets (removing digits and specialcharacters).\n",
        "    - b. Extract words starting with a vowel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9c9b492d-7dfb-492d-9182-135ac4aedc5b",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c9b492d-7dfb-492d-9182-135ac4aedc5b",
        "outputId": "0849669f-d73d-4470-e7a8-c669f3acbb75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "words (>5) letters:   ['simple', 'comforting', 'brings', 'warmth', 'tender', 'potatoes', 'cooked', 'flavorful', 'spices', 'turmeric', 'coriander', 'hearty', 'satisfying', 'potatoes', 'spices', 'creating', 'deliciously', 'aromatic', 'perfectly', 'paratha', 'whether', 'version', 'chilies', 'milder', 'sweetness', 'versatility', 'comforting'] \n",
            "\n",
            "\n",
            "Numbers :   [] \n",
            "\n",
            "\n",
            "Capitalized :   ['Aloo', 'Made', 'The', 'Whether', 'It'] \n",
            "\n",
            "\n",
            "Only alpha words:   ['aloo', 'ki', 'sabzi', 'is', 'a', 'simple', 'yet', 'comforting', 'dish', 'that', 'brings', 'warmth', 'to', 'any', 'meal', 'made', 'with', 'tender', 'potatoes', 'cooked', 'in', 'a', 'flavorful', 'blend', 'of', 'spices', 'like', 'cumin', 'turmeric', 'and', 'coriander', 'it', 's', 'both', 'hearty', 'and', 'satisfying', 'the', 'potatoes', 'soak', 'up', 'all', 'the', 'spices', 'creating', 'a', 'deliciously', 'aromatic', 'dish', 'that', 'pairs', 'perfectly', 'with', 'roti', 'paratha', 'or', 'rice', 'whether', 'its', 'a', 'spicy', 'version', 'with', 'green', 'chilies', 'or', 'a', 'milder', 'one', 'with', 'a', 'touch', 'of', 'sweetness', 'aloo', 'ki', 'sabzi', 'is', 'loved', 'for', 'its', 'versatility', 'and', 'comforting', 'taste', 'it', 's', 'a', 'dish', 'that', 'feels', 'like', 'home', 'in', 'every', 'bite'] \n",
            "\n",
            "Only alpha words:\n",
            " ['aloo', 'ki', 'sabzi', 'is', 'a', 'simple', 'yet', 'comforting', 'dish', 'that', 'brings', 'warmth', 'to', 'any', 'meal', 'made', 'with', 'tender', 'potatoes', 'cooked', 'in', 'a', 'flavorful', 'blend', 'of', 'spices', 'like', 'cumin', 'turmeric', 'and', 'coriander', 'both', 'hearty', 'and', 'satisfying', 'the', 'potatoes', 'soak', 'up', 'all', 'the', 'spices', 'creating', 'a', 'deliciously', 'aromatic', 'dish', 'that', 'pairs', 'perfectly', 'with', 'roti', 'paratha', 'or', 'rice', 'whether', 'its', 'a', 'spicy', 'version', 'with', 'green', 'chilies', 'or', 'a', 'milder', 'one', 'with', 'a', 'touch', 'of', 'sweetness', 'aloo', 'ki', 'sabzi', 'is', 'loved', 'for', 'its', 'versatility', 'and', 'comforting', 'taste', 'a', 'dish', 'that', 'feels', 'like', 'home', 'in', 'every', 'bite']\n",
            "\n",
            "Words starting with vowels:\n",
            " ['aloo', 'is', 'a', 'any', 'in', 'a', 'of', 'and', 'it', 'and', 'up', 'all', 'a', 'aromatic', 'or', 'its', 'a', 'or', 'a', 'one', 'a', 'of', 'aloo', 'is', 'its', 'and', 'it', 'a', 'in', 'every']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "letter_5 = re.findall(r'\\b\\w{6,}\\b', no_punc)\n",
        "print(\"words (>5) letters:  \", letter_5 ,'\\n')\n",
        "# \\w{6,} = Match any word (\\w) with 6 or more characters.\n",
        "\n",
        "numb = re.findall(r'\\b\\d+\\b', no_punc)\n",
        "print(\"\\nNumbers :  \", numb,'\\n')\n",
        "# \\d+ = One or more digits (0-9)\n",
        "\n",
        "caps = re.findall(r'\\b[A-Z][a-zA-Z]*\\b', sentence)\n",
        "print(\"\\nCapitalized :  \", caps, '\\n')\n",
        "# [a-zA-Z]* = Followed by zero or more letters (upper/lower case).\n",
        "\n",
        "alphas = re.findall(r'\\b[a-zA-Z]+\\b', no_punc)\n",
        "print(\"\\nOnly alpha words:  \", alphas, '\\n')\n",
        "# [a-zA-Z]+ = One or more alphabet characters (no digits or symbols).\n",
        "\n",
        "# text into words\n",
        "words_list = no_punc.split()\n",
        "only_alpha_words = [w for w in words_list if w.isalpha()]\n",
        "print(\"Only alpha words:\\n\", only_alpha_words)\n",
        "\n",
        "vowelss = [w for w in alphas if w[0].lower() in 'aeiou']\n",
        "print(\"\\nWords starting with vowels:\\n\", vowelss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "385a84f3-f396-475f-ab62-cdf9f3ea6ed7",
      "metadata": {
        "id": "385a84f3-f396-475f-ab62-cdf9f3ea6ed7"
      },
      "source": [
        "# Q4. Custom Tokenization & Regex-based Text Cleaning\n",
        "\n",
        "## Steps:\n",
        "\n",
        "1. **Input**: Use the original text from **Question 1**.\n",
        "\n",
        "2. **Custom Tokenization Function Requirements**:\n",
        "   \n",
        "   - **(a)** Remove punctuation and special symbols, **but keep contractions** (e.g., `\"isn't\"` should remain `\"isn't\"` and not split into `\"is\"` and `\"n't\"`).\n",
        "   \n",
        "   - **(b)** Handle **hyphenated words** as **single tokens** (e.g., `\"state-of-the-art\"` should remain as one token).\n",
        "   \n",
        "   - **(c)** **Tokenize numbers separately**, but **keep decimal numbers intact** (e.g., `\"3.14\"` should remain `\"3.14\"`).\n",
        "\n",
        "3. **Regex Substitutions** (using `re.sub`):\n",
        "\n",
        "   - **(a)** Replace **email addresses** with the placeholder `<EMAIL>`.\n",
        "   \n",
        "   - **(b)** Replace **URLs** with the placeholder `<URL>`.\n",
        "   \n",
        "   - **(c)** Replace **phone numbers** (formats like `123-456-7890` or `+91 9876543210`) with the plceho-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\n",
        "placeholder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1ba0f752-0f2b-4d70-b76d-b5b16fd6cc2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ba0f752-0f2b-4d70-b76d-b5b16fd6cc2a",
        "outputId": "4789dde0-a5ff-468c-b7bb-0428abb50587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Custom Tokens:   ['Aloo', 'ki', 'sabzi', 'is', 'a', 'simple', 'yet', 'comforting', 'dish', 'that', 'brings', 'warmth', 'to', 'any', 'meal', 'Made', 'with', 'tender', 'potatoes', 'cooked', 'in', 'a', 'flavorful', 'blend', 'of', 'spices', 'like', 'cumin', 'turmeric', 'and', 'coriander', 'its', 'both', 'hearty', 'and', 'satisfying', 'The', 'potatoes', 'soak', 'up', 'all', 'the', 'spices', 'creating', 'a', 'deliciously', 'aromatic', 'dish', 'that', 'pairs', 'perfectly', 'with', 'roti', 'paratha', 'or', 'rice', 'Whether', \"it's\", 'a', 'spicy', 'version', 'with', 'green', 'chilies', 'or', 'a', 'milder', 'one', 'with', 'a', 'touch', 'of', 'sweetness', 'aloo', 'ki', 'sabzi', 'is', 'loved', 'for', 'its', 'versatility', 'and', 'comforting', 'taste', 'Its', 'a', 'dish', 'that', 'feels', 'like', 'home', 'in', 'every', 'bite'] \n",
            "\n",
            "\n",
            "Text after Regex Substitutions:\n",
            " \n",
            "Aloo ki sabzi is a simple yet comforting dish that brings warmth to any meal.\n",
            " Made with tender potatoes cooked in a flavorful blend of spices like cumin, turmeric, and coriander, it’s both hearty and satisfying. \n",
            " The potatoes soak up all the spices, creating a deliciously aromatic dish that pairs perfectly with roti, paratha, or rice.\n",
            "  Whether it's a spicy version with green chilies or a milder one with a touch of sweetness, aloo ki sabzi is loved for its versatility and comforting taste.\n",
            " It’s a dish that feels like home in every bite!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Custom Tokenizer\n",
        "def custom_tokens(text):\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s\\-']\", \"\", text)\n",
        "    tokens = re.findall(r\"\\b\\w+(?:[-']\\w+)*\\b\", text)\n",
        "    # ?: = Non-capturing group (just groups the pattern, doesn't save it separately)\n",
        "    return tokens\n",
        "customs = custom_tokens(sentence)\n",
        "print(\"\\nCustom Tokens:  \", customs, '\\n')\n",
        "\n",
        "\n",
        "text_replaced = re.sub(r'\\b[\\w.-]+?@\\w+?\\.\\w+?\\b', '<EMAIL>', sentence)\n",
        "# [\\w.-]+?\tOne or more word characters (a-z, A-Z, 0-9, _), dot ., or hyphen - (email username part)\n",
        "# \\w+?\tDomain name (like gmail) and then the domain extension\n",
        "\n",
        "text_replaced = re.sub(r'http[s]?://\\S+', '<URL>', text_replaced)\n",
        "# http[s]?\tMatch http or https (the s? means \"s\" is optional)\n",
        "# \\S+\tMatch one or more non-space characters (the URL itself)\n",
        "\n",
        "text_replaced = re.sub(r'(\\+?\\d{1,2}\\s?)?(\\d{3}[-\\s]?\\d{3}[-\\s]?\\d{4})', '<PHONE>', text_replaced)\n",
        "# (\\+?\\d{1,2}\\s?)? – The entire pattern is optional due to the outer ()\n",
        "# \\+?  Matches an optional plus sign (+)\n",
        "# \\d{1,2}  Matches 1 or 2 digits (0-9)\n",
        "# \\s?   Matches an optional whitespace character (like a space or tab)\n",
        "# [-\\s]?   Optional hyphen or space (separator)\n",
        "# \\d{4}\tExactly 4 digits (last part)\n",
        "print(\"\\nText after Regex Substitutions:\\n\", text_replaced)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a6d4c7e-1fef-4ce9-ae77-e8e8b93897e2",
      "metadata": {
        "id": "4a6d4c7e-1fef-4ce9-ae77-e8e8b93897e2"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}